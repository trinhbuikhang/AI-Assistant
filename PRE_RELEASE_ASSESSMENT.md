# Pre-Release Assessment ‚Äî AI Assistant v1.0

**Assessor:** Senior product engineer / UX reviewer  
**Date:** 2026-02-26  
**Scope:** Local AI Assistant (FastAPI + single HTML, Ollama backend)

---

## PART 1 ‚Äî RELEASE READINESS CHECKLIST

### Installation & First Run

| Item | Answer | Notes |
|------|--------|------|
| Can a non-technical user install and run in under 5 minutes? | **PARTIAL** | Yes if they follow README (venv + pip + python main.py), but ‚Äúnon-technical‚Äù may struggle with venv and ‚ÄúStart Ollama first.‚Äù |
| Clear README with step-by-step setup? | **YES** | Install, Run, Troubleshooting are clear; venv steps for Win/Mac/Linux. |
| All dependencies in requirements.txt with pinned versions? | **NO** | Uses `>=` (e.g. `fastapi>=0.110.0`). No pinned versions; risk of breakage on future releases. |
| App checks if Ollama is installed and running on startup? | **PARTIAL** | `check_ollama_running()` runs after server is ready; **only prints a warning to terminal**. No in-browser message; user may not see it. |
| App checks if at least one model is downloaded? | **NO** | No startup check. UI shows ‚Äú(No models)‚Äù in dropdown but **does not tell user to run `ollama pull <model>`** in-app. |
| Single command start: `python main.py`? | **YES** | Starts server, finds port, opens browser. |
| Works on Windows, macOS, Linux without modification? | **YES** | Paths use `Path`, port bind to 127.0.0.1; no OS-specific code in main path. |
| All file paths relative (no hardcoded machine paths)? | **YES** | `config.py` uses `Path(__file__).resolve().parent`; `config_manager` uses same pattern. |

### Stability

| Item | Answer | Notes |
|------|--------|------|
| Tested on slow/low-RAM machine? | **UNKNOWN** | Not verifiable from codebase. |
| Recovers gracefully if Ollama crashes mid-conversation? | **PARTIAL** | Stream raises exception ‚Üí server sends `type: error` and appends error to session; **no automatic reconnect or ‚ÄúRetry‚Äù**. |
| Handles being left open for hours? | **YES** | Session capped at `MAX_SESSION_MESSAGES`; upload store capped; no obvious leak. |
| Safe to close browser tab and reopen? | **PARTIAL** | **Reopen = new WebSocket = new session.** Previous messages are **lost** (sessions are in-memory, keyed by `ws_id`). |
| Ctrl+C shuts down cleanly? | **YES** | Main thread waits on `input()`; daemon server thread exits with process; no explicit cleanup of Ollama if app started it (main.py doesn‚Äôt start Ollama). |
| Port conflict handled? | **YES** | `_find_free_port()` tries 8000, 8001, 8002; clear error if none available. |

### Core Feature Completeness

| Item | Answer | Notes |
|------|--------|------|
| Chat works reliably with streaming? | **YES** | WebSocket streams tokens; `type: token` / `done`; typing indicator; markdown/code after done. |
| Conversation history persists across refresh? | **NO** | **Sessions are in-memory per WebSocket.** Refresh = new connection = empty chat. |
| Multiple conversations created and switched? | **NO** | Sidebar shows only **one** ‚ÄúCurrent‚Äù conversation (today). No list of past chats; no persistence. |
| Model change without restart? | **YES** | Dropdown in sidebar; selection sent per message; no restart. |
| File upload works and content sent to AI? | **YES** | `/upload` ‚Üí file_id; chat payload includes file_id/file_ids; server builds user message with file text/summary. |
| New Chat clears context and starts fresh? | **YES** | Clear chat clears DOM and local state; new WebSocket connection gets new server session. |
| User can stop/cancel generation mid-stream? | **NO** | **No stop button.** Backend has `stop_event` in thread but **no WebSocket message type to trigger it**; UI has no ‚ÄúStop‚Äù control. |

### Distribution Readiness

| Item | Answer | Notes |
|------|--------|------|
| .gitignore present (if sharing via git)? | **YES** | venv, __pycache__, .env, app.log, uploads_temp, chat_history, IDE, OS. |
| No sensitive data in codebase? | **YES** | No API keys; OLLAMA_BASE is localhost; config.json is local and gitignored if desired (not in .gitignore currently ‚Äî consider adding). |
| Folder structure clean (no temp/debug/test)? | **PARTIAL** | `config.json`, `app.log` may be present; `test_folder_summary.py` in repo. Fine for dev; for zip distribution exclude logs/config or document. |
| Packaged as .exe / .app for non-Python users? | **NO** | README targets ‚Äúrun python main.py.‚Äù No PyInstaller/spec or similar. |
| App size reasonable? | **YES** | No model files in repo; dependencies are normal for stack. |

---

## PART 2 ‚Äî USER EXPERIENCE FINAL PASS

### First Impression (first 10 seconds)

| Item | Answer | Notes |
|------|--------|------|
| Welcome screen inviting and self-explanatory? | **YES** | Greeting, ‚ÄúPowered by [model],‚Äù suggestion chips with clear labels. |
| User immediately understands what to do? | **YES** | Placeholder ‚ÄúMessage...‚Äù, chips suggest actions, attach/folder hints. |
| App name/branding clear and consistent? | **YES** | ‚ÄúAI Assistant‚Äù in sidebar, header, title, welcome. |
| Feels fast and responsive on first load? | **YES** | Static HTML + CSS; WebSocket connect and /api/models are quick. |

### Core Interaction Loop

| Item | Answer | Notes |
|------|--------|------|
| Type ‚Üí send ‚Üí see response frictionless? | **YES** | Enter to send; send button; streaming into bubble. |
| Streaming text smooth and readable? | **YES** | Tokens append to one element; then full markdown render on `done`. |
| Code blocks formatted with syntax highlighting? | **YES** | highlight.js after render; `.copy-btn` on `pre`. |
| Markdown renders correctly? | **YES** | marked + DOMPurify; bold, lists, headers, links. |
| Chat scroll behavior natural? | **YES** | `scrollToBottom()` on token/done; `userScrolledUp` avoids fighting user. |

### Information Density

| Item | Answer | Notes |
|------|--------|------|
| Current model obvious? | **YES** | Header badge ‚Äúmodel name‚Äù + sidebar dropdown. |
| ‚ÄúThinking‚Äù vs done clear? | **YES** | Typing dots while streaming; spinner on send; dots removed on `done`. |
| How many conversations in sidebar clear? | **N/A** | Only one ‚Äúconversation‚Äù (current) shown; no count. |
| Confusing UI element needing tooltip? | **PARTIAL** | ‚ÄúOr enter folder path‚Äù is clear; ‚ãØ on conversation might benefit ‚ÄúClear chat.‚Äù |

### Empty / Edge States

| Item | Answer | Notes |
|------|--------|------|
| Empty sidebar looks intentional? | **PARTIAL** | When no messages, sidebar shows ‚ÄúToday‚Äù + one ‚ÄúCurrent‚Äù item with placeholder title; could look odd before first message. |
| Very long AI response: layout ok? | **YES** | `.message-content` flows; scroll on messagesWrap. |
| Very long user message: bubble ok? | **YES** | Same max-width 72%; wraps. |
| Unsupported file type: friendly error? | **PARTIAL** | **Upload (button):** server returns 400 with ‚ÄúUnsupported format. Use: .pdf, .docx, .txt, .csv‚Äù ‚Äî good. **Drag-drop:** only allowed extensions are uploaded; **unsupported files are skipped with no message** ‚Äî user may think nothing happened. |

---

## PART 3 ‚Äî MISSING FEATURES ASSESSMENT

| Feature | Priority | Effort | Status |
|---------|----------|--------|--------|
| Export conversation (MD/PDF/TXT) | üü† High | Medium | **Missing** |
| Rename conversation (click to edit title) | üü† High | Easy | **Partial** (title auto-set from first message; no edit) |
| Delete individual conversation | üü† High | Easy | **N/A** (only one session shown) |
| Search conversation history | üü° Nice | Medium | **Missing** |
| Pin conversations to top | üü° Nice | Easy | **N/A** |
| Duplicate conversation | üü° Nice | Medium | **Missing** |
| **Stop/cancel generation button** | üî¥ **Must-have** | **Easy** | **Missing** (backend has stop_event; no WS + UI) |
| Regenerate last response | üü† High | Medium | **Missing** |
| Edit message and regenerate | üü° Nice | Medium | **Missing** |
| Copy individual message | üü° Nice | Easy | **Missing** |
| Copy code block button | ‚úÖ | ‚Äî | **Already exists** |
| System prompt / persona | ‚úÖ | ‚Äî | **Already exists** (Settings) |
| Temperature / creativity | ‚úÖ | ‚Äî | **Already exists** (Settings) |
| Context window indicator | üü° Nice | Hard | **Missing** |
| Drag and drop files onto chat | ‚úÖ | ‚Äî | **Already exists** |
| Preview uploaded file before send | üü° Nice | Medium | **Partial** (chips show name only) |
| Multiple file uploads in one message | ‚úÖ | ‚Äî | **Already exists** |
| Image upload (vision) | üü° Nice | Medium | **Missing** |
| Paste image from clipboard | üü° Nice | Medium | **Missing** |
| Prompt templates / quick prompts | ‚úÖ | ‚Äî | **Partial** (suggestion chips) |
| Keyboard shortcuts (Ctrl+N, Ctrl+K) | üü† High | Easy | **Missing** (Enter to send only) |
| Dark/light theme toggle | üü° Nice | Easy | **Missing** (dark only) |
| Font size adjustment | üü° Nice | Easy | **Missing** |
| Chat with specific document (RAG-lite) | üü° Nice | Medium | **Partial** (file attach) |
| Settings page (default model, Ollama URL, theme, etc.) | ‚úÖ | ‚Äî | **Exists** (modal: system prompt, temp, max tokens, default model) |
| Custom Ollama server URL | üü† High | Medium | **Missing** (hardcoded localhost:11434) |
| Model info on hover (params, context) | üü° Nice | Medium | **Missing** |
| Token count / response time per message | üü° Nice | Medium | **Missing** |
| Raw JSON view of conversation | üü° Nice | Easy | **Missing** |
| System prompt override per conversation | üü° Nice | Medium | **Missing** |
| API key for cloud models as fallback | üü° Nice | Hard | **Missing** |

---

## PART 4 ‚Äî COMPETITIVE BASELINE CHECK

| Expectation | Status |
|-------------|--------|
| Markdown rendering in responses | ‚úÖ Yes |
| Code syntax highlighting | ‚úÖ Yes |
| Copy button on code blocks | ‚úÖ Yes |
| **Stop generation button** | ‚ùå **No** ‚Äî major gap |
| Conversation history sidebar | ‚ö†Ô∏è Partial (single ‚Äúcurrent‚Äù only; no persisted list) |
| New chat button | ‚úÖ Yes |
| Model selector | ‚úÖ Yes |
| Enter to send | ‚úÖ Yes |
| Auto-focus on input when page loads | ‚úÖ Yes (`inputText.focus()`) |
| Scroll to bottom on new message | ‚úÖ Yes |
| Timestamp on messages | ‚ùå No |
| Clear user vs AI distinction | ‚úÖ Yes (bubbles, alignment, avatar) |
| Responsive (window sizes) | ‚úÖ Yes (mobile.css, sidebar toggle) |

---

## PART 5 ‚Äî QUICK WIN IMPROVEMENTS

**[QUICK WIN #1]**  
**What:** Add a ‚ÄúStop‚Äù button that appears while streaming and sends a stop signal to the server.  
**Why:** Users expect to cancel long or wrong generations; competitive baseline.  
**How:** In `index.html`: show a stop button (replace or beside send) when `streaming === true`; on click set a flag and send `{ type: "stop" }` over WebSocket. In `server.py`: handle `msg_type == "stop"` by setting a per-ws_id event that `stream_response` (or the thread) checks; ai_service already uses `stop_event`. Wire the same event from server to the generator.  
**Effort:** ~1‚Äì1.5 hrs  

**[QUICK WIN #2]**  
**What:** When Ollama is not running or models list is empty, show an in-browser banner or welcome message with next steps.  
**Why:** Terminal warning is easy to miss; in-app message reaches all users.  
**How:** Add a `/api/health/ollama` or extend `/api/models` to return `{ models: [], ollama_ok: bool }`. In frontend, if `!ollama_ok` or `models.length === 0`, show a dismissible banner: ‚ÄúStart Ollama and run: ollama pull &lt;model&gt;‚Äù with link to ollama.ai.  
**Effort:** ~45 min  

**[QUICK WIN #3]**  
**What:** Pin dependency versions in requirements.txt (e.g. fastapi==0.110.0, uvicorn==0.29.0).  
**Why:** Reproducible installs; fewer ‚Äúworks on my machine‚Äù issues.  
**How:** `pip freeze` from current venv (or pick last known-good versions) and replace `>=` with `==` in requirements.txt.  
**Effort:** ~15 min  

**[QUICK WIN #4]**  
**What:** When user drops only unsupported file types, show a toast: ‚ÄúSupported: .pdf, .docx, .txt, .csv‚Äù.  
**Why:** Avoids silent failure and confusion.  
**How:** In the drop handler, after filtering by `allowed`, if `list.length === 0` and `droppedFiles.length > 0`, call `showError('Supported formats: .pdf, .docx, .txt, .csv')`.  
**Effort:** ~15 min  

**[QUICK WIN #5]**  
**What:** Keyboard shortcut: Ctrl+N (or Cmd+N) for New Chat.  
**Why:** Power users expect it; matches common apps.  
**How:** `document.addEventListener('keydown', function(e) { if ((e.ctrlKey || e.metaKey) && e.key === 'n') { e.preventDefault(); clearChat(); } })`.  
**Effort:** ~15 min  

**[QUICK WIN #6]**  
**What:** Add a short timestamp (e.g. ‚Äú2:34 PM‚Äù) to each message bubble.  
**Why:** Helps users see when something was said; expected in chat UIs.  
**How:** In `addMessage()`, append a `<span class="message-time">` with `new Date().toLocaleTimeString(...)`. Style small and muted in message-bubbles.css.  
**Effort:** ~30 min  

**[QUICK WIN #7]**  
**What:** After ‚Äú(No models)‚Äù is loaded, set welcome sub text to ‚ÄúInstall a model: run ollama pull &lt;model&gt; in terminal.‚Äù  
**Why:** In-app guidance without extra API.  
**How:** In `loadModels()` when `list.length === 0`, set `welcomeSub.textContent = 'Install a model: run ollama pull <model> in terminal.'` (and optionally show same in a small hint under model dropdown).  
**Effort:** ~15 min  

**[QUICK WIN #8]**  
**What:** Auto-focus input after sending a message (so user can type next message immediately).  
**Why:** Smoother flow; user doesn‚Äôt have to click back.  
**How:** In `sendMessage()` after `addMessage(‚Ä¶)` and clearing input, call `inputText.focus()`. On `done` handler, optionally focus input again.  
**Effort:** ~10 min  

**[QUICK WIN #9]**  
**What:** Add `config.json` to .gitignore (if not already) so local settings aren‚Äôt committed.  
**Why:** Avoid sharing personal system prompts/default model.  
**How:** Add `config.json` to .gitignore. Document in README that config is local.  
**Effort:** ~5 min  

**[QUICK WIN #10]**  
**What:** ‚ÄúCopy‚Äù for entire assistant message (not just code blocks).  
**Why:** Users often want to copy the full reply.  
**How:** Add a small copy icon/button on each assistant bubble (e.g. on hover); onclick copy `contentEl.innerText` or equivalent to clipboard and show ‚ÄúCopied!‚Äù briefly.  
**Effort:** ~30 min  

---

## PART 6 ‚Äî RELEASE DECISION

### Verdict: **üü° RELEASE WITH CAVEATS**

The app is **releasable** for a v1.0 ‚Äúlocal AI assistant‚Äù if you **communicate limitations clearly**. It is not ‚Äúnot ready,‚Äù but missing a **stop button** and **persistent conversation history** will surprise users used to ChatGPT/Claude.

**Caveats to communicate (e.g. in README or release notes):**

1. **No stop button** ‚Äî You cannot cancel a response once it‚Äôs generating; we‚Äôll add it soon.
2. **Conversations don‚Äôt persist** ‚Äî Closing the tab or refreshing clears the current chat; we don‚Äôt yet save history to disk.
3. **Single conversation** ‚Äî The sidebar shows only the current chat; no list of past conversations yet.
4. **Ollama required** ‚Äî Must be installed and running; if no models, run `ollama pull <model>` (document this clearly).

**Optional but recommended before calling it v1.0:** Implement **Quick Win #1 (Stop button)** so the competitive baseline is met.

---

### Release checklist (before sharing)

- [ ] Pin or document dependency versions (Quick Win #3).
- [ ] Add in-app hint when no models / Ollama not running (Quick Win #2 and/or #7).
- [ ] Add `config.json` to .gitignore if you don‚Äôt want to ship local config (Quick Win #9).
- [ ] README: state ‚ÄúNo stop button yet‚Äù and ‚ÄúConversations are not saved across refresh.‚Äù
- [ ] README: add ‚ÄúFirst time? Run `ollama pull llama3.2` (or another model) before using.‚Äù
- [ ] Test run: fresh venv, `pip install -r requirements.txt`, `python main.py`, send a message and refresh (confirm chat is empty).
- [ ] If packaging as zip: exclude `venv_web/`, `app.log`, `uploads_temp/`, `config.json` (or document that config is user-specific).

---

### Recommended post-release roadmap

**v1.1 (quick wins, 1‚Äì2 days)**  
- Stop generation button (Quick Win #1).  
- In-app Ollama/models guidance (Quick Win #2, #7).  
- Pinned requirements (Quick Win #3).  
- Unsupported file drop message (Quick Win #4).  
- Ctrl+N for new chat (Quick Win #5).  
- Message timestamps (Quick Win #6).  
- Copy full message (Quick Win #10).  

**v1.2 (medium, ~1 week)**  
- Persist conversations (e.g. localStorage or server-side with session id + file/DB).  
- Conversation list in sidebar (load/save/switch).  
- Rename/delete conversation.  
- Regenerate last response.  
- Custom Ollama URL in Settings.  

**v2.0 (future)**  
- Export conversation (Markdown/PDF).  
- Search in history.  
- Light/dark theme toggle.  
- Optional cloud API fallback (OpenAI/Anthropic/Groq) when Ollama unavailable.  

---

### Suggested elevator pitch (README or sharing)

**AI Assistant** is a **local, private chat app** that runs entirely on your machine. Point it at **Ollama** and use any model you‚Äôve pulled‚Äîno accounts, no cloud. **One command** (`python main.py`) starts the server and opens the app in your browser: streaming chat, file and folder uploads, and a dark UI with Markdown and code highlighting. It‚Äôs for anyone who wants a **simple, offline-first** assistant without sending data to the cloud.

---

## PART 7 ‚Äî IMPLEMENT APPROVED QUICK WINS

**No Quick Wins have been implemented in this assessment.**

After you decide which Quick Wins to approve, I will:

- Implement **one at a time**.
- Show the exact code change (diff-style).
- Explain what changed and confirm no other parts of the app are broken.
- Wait for your approval before moving to the next.

Tell me which number(s) you want (e.g. ‚ÄúDo #1 and #3‚Äù), and we‚Äôll start with the first one.
